{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer         \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer     \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Score'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "2     29769\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "score = data['Score']\n",
    "temp = score.map(partition)\n",
    "data['Score'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:100000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data['Text']\n",
    "data_Y = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Y.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'through', 'now', 'where', 'be', 'very', \"weren't\", \"don't\", 'hadn', 'were', 'didn', 'for', 'yours', 'couldn', 'of', 'these', 'ourselves', 'at', 'mightn', 'myself', 'ours', 'him', 'by', \"it's\", 'haven', 'doesn', \"wasn't\", 'in', 'll', 'further', 'won', 'all', 'them', 'whom', 'had', 'and', 'my', 'when', 'those', 'into', 'nor', 'an', 'only', \"should've\", \"shan't\", 'am', 'own', 'between', 'their', 'i', 'same', 'so', 'm', 'y', 'does', 'itself', \"shouldn't\", \"aren't\", 'from', 'd', 'most', 'yourselves', 'has', 'did', 'such', 'weren', 'it', 'theirs', 'up', 'some', \"mightn't\", 'out', 'more', \"isn't\", \"you're\", 'who', 'wasn', 'after', 'we', 'against', 't', 'themselves', 'how', 'than', 're', 'her', 'can', 'any', \"needn't\", 'our', 'not', \"mustn't\", 'over', 'should', \"that'll\", 'just', 'isn', 'is', 'during', \"doesn't\", 'until', 'while', 'as', 'what', \"she's\", 'ma', 'about', \"couldn't\", 'under', \"you'll\", 'the', 'each', 'don', 'his', 'are', 've', 'shan', \"you've\", 'this', 's', 'was', \"wouldn't\", 'doing', 'below', 'ain', 'that', 'if', 'having', 'your', 'then', 'there', 'why', 'me', 'once', 'o', 'down', 'which', 'both', 'or', 'too', 'yourself', 'above', 'but', 'a', 'off', \"hasn't\", 'on', \"haven't\", 'herself', 'because', 'have', 'himself', 'will', 'here', 'again', \"didn't\", 'been', 'do', 'needn', 'they', \"won't\", 'he', 'its', 'no', \"hadn't\", 'mustn', 'few', 'hers', 'aren', 'other', \"you'd\", 'to', 'hasn', 'before', 'wouldn', 'shouldn', 'you', 'she', 'with'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def cleanhtml(sentence):\n",
    "    sentence = sentence.lower()                                       # Converting to lowercase\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)                          # Cleaning Html tags\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def cleanpunc(sentence):\n",
    "    sentence = re.sub('[^A-Za-z0-9]+', '', sentence)                # Cleaning punctuations\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "str1 = ''\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "i = 0\n",
    "for sentence in data_X:\n",
    "    words = []\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for word in sentence.split():\n",
    "        word = cleanpunc(word)\n",
    "        if((word.isalpha()) & (len(word) > 2)): \n",
    "            if word not in stop:\n",
    "                s = snow.stem(word).encode('utf8')\n",
    "                words.append(s)\n",
    "                if (data_Y.values)[i] == 'positive': \n",
    "                    positive_words.append(s) #list of all words used to describe positive reviews.\n",
    "                if(data_Y.values)[i] == 'negative':\n",
    "                    negative_words.append(s) #list of all words used to describe negative reviews.\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "                continue\n",
    "    str1 = b\" \".join(words)\n",
    "    temp.append(str1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'bought sever vital can dog food product found good qualiti product look like stew process meat smell better labrador finicki appreci product better',\n",
       " b'product arriv label jumbo salt peanutsth peanut actual small size unsalt sure error vendor intend repres product jumbo',\n",
       " b'confect around centuri light pillowi citrus gelatin nut case filbert cut tini squar liber coat powder sugar tini mouth heaven chewi flavor high recommend yummi treat familiar stori lewi lion witch wardrob treat seduc edmund sell brother sister witch',\n",
       " b'look secret ingredi robitussin believ found got addit root beer extract order good made cherri soda flavor medicin',\n",
       " b'great taffi great price wide assort yummi taffi deliveri quick taffi lover deal',\n",
       " b'got wild hair taffi order five pound bag taffi enjoy mani flavor watermelon root beer melon peppermint grape etc complaint bit much redblack licoriceflavor piec particular favorit kid husband last two week would recommend brand taffi delight treat',\n",
       " b'saltwat taffi great flavor soft chewi candi individu wrap well none candi stuck togeth happen expens version fraling would high recommend candi serv beachthem parti everyon love',\n",
       " b'taffi good soft chewi flavor amaz would definit recommend buy satisfi',\n",
       " b'right most sprout cat eat grass love rotat around wheatgrass rye',\n",
       " b'healthi dog food good digest also good small puppi dog eat requir amount everi feed']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(data_X, data_Y)), columns = ['Text', 'Sentiment'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f79dccb8755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/cleaned_file.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_pickle('data/cleaned_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/cleaned_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'bought sever vital can dog food product foun...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'product arriv label jumbo salt peanutsth pea...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'confect around centuri light pillowi citrus ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'look secret ingredi robitussin believ found ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'great taffi great price wide assort yummi ta...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  b'bought sever vital can dog food product foun...  positive\n",
       "1  b'product arriv label jumbo salt peanutsth pea...  negative\n",
       "2  b'confect around centuri light pillowi citrus ...  positive\n",
       "3  b'look secret ingredi robitussin believ found ...  negative\n",
       "4  b'great taffi great price wide assort yummi ta...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'confect around centuri light pillowi citrus gelatin nut case filbert cut tini squar liber coat powder sugar tini mouth heaven chewi flavor high recommend yummi treat familiar stori lewi lion witch wardrob treat seduc edmund sell brother sister witch'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Text\"]\n",
    "y = df[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 2), (100000,), (100000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (60000,) (20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "X_1 = X[:math.ceil(len(df)*0.8)]\n",
    "y_1 = y[:math.ceil(len(df)*0.8)]\n",
    "X_test = X[math.ceil(len(df)*0.8):]\n",
    "y_test = y[math.ceil(len(df)*0.8):]\n",
    "\n",
    "X_train = X_1[:math.ceil(len(df)*0.6)]\n",
    "y_train = y_1[:math.ceil(len(df)*0.6)]\n",
    "X_cv = X_1[math.ceil(len(df)*0.6):]\n",
    "y_cv = y_1[math.ceil(len(df)*0.6):]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_cv.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = []\n",
    "for sentence in X_train:\n",
    "    words = []\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for word in sentence.split():\n",
    "        word = cleanpunc(word)\n",
    "        if(word.isalpha()):    \n",
    "            words.append(word)\n",
    "        else:\n",
    "            continue \n",
    "    list_of_sentence.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bought', 'sever', 'vital', 'can', 'dog', 'food', 'product', 'found', 'good', 'qualiti', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'labrador', 'finicki', 'appreci', 'product', 'better'], ['product', 'arriv', 'label', 'jumbo', 'salt', 'peanutsth', 'peanut', 'actual', 'small', 'size', 'unsalt', 'sure', 'error', 'vendor', 'intend', 'repres', 'product', 'jumbo'], ['confect', 'around', 'centuri', 'light', 'pillowi', 'citrus', 'gelatin', 'nut', 'case', 'filbert', 'cut', 'tini', 'squar', 'liber', 'coat', 'powder', 'sugar', 'tini', 'mouth', 'heaven', 'chewi', 'flavor', 'high', 'recommend', 'yummi', 'treat', 'familiar', 'stori', 'lewi', 'lion', 'witch', 'wardrob', 'treat', 'seduc', 'edmund', 'sell', 'brother', 'sister', 'witch'], ['look', 'secret', 'ingredi', 'robitussin', 'believ', 'found', 'got', 'addit', 'root', 'beer', 'extract', 'order', 'good', 'made', 'cherri', 'soda', 'flavor', 'medicin'], ['great', 'taffi', 'great', 'price', 'wide', 'assort', 'yummi', 'taffi', 'deliveri', 'quick', 'taffi', 'lover', 'deal']]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_sentence[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(list_of_sentence, min_count = 5, size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000001843032E5C0>\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought', 'sever', 'vital', 'can', 'dog', 'food', 'product', 'found', 'good', 'qualiti', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'labrador', 'finicki', 'appreci', 'arriv', 'label', 'jumbo', 'salt', 'peanut', 'actual', 'small', 'size', 'unsalt', 'sure', 'error', 'vendor', 'intend', 'repres', 'confect', 'around', 'centuri', 'light', 'citrus', 'gelatin', 'nut', 'case', 'cut', 'tini', 'squar', 'liber', 'coat', 'powder', 'sugar', 'mouth']\n"
     ]
    }
   ],
   "source": [
    "print(w2v_vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:32<00:00, 650.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "corpus_embedding = []\n",
    "for sentence in tqdm(X_train):\n",
    "#     print(sentence)\n",
    "#     break\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    count_words = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            sentence_embedding += embedding\n",
    "            count_words += 1\n",
    "    if count_words != 0:\n",
    "        sentence_embedding /= count_words\n",
    "    corpus_embedding.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.61739673,  0.12484459, -0.06689609,  0.45843966,  0.21517037,\n",
       "        -0.38145365, -0.22890107, -0.24621044,  0.18322493, -0.64715211,\n",
       "         0.47312825,  0.1212746 , -0.20917033,  0.07304018,  0.1917164 ,\n",
       "        -0.34190255, -0.0740689 ,  0.23553804, -0.19848122, -0.09747929,\n",
       "        -0.40971219, -0.50984567,  0.7398974 , -0.17426051, -0.38910081,\n",
       "         0.14594186, -0.15753686,  0.10157121, -0.83091147, -0.41507436,\n",
       "        -0.23850454, -1.0695665 ,  0.02278422,  0.3257149 , -0.17303633,\n",
       "        -0.6618879 , -0.17315514,  0.48021129,  0.80094977, -0.67133373,\n",
       "         0.80095113, -0.01180873, -0.0398592 , -0.5492462 , -0.45810671,\n",
       "         0.39854293,  1.05066226, -0.4883309 ,  0.86311581, -0.23029786]),\n",
       " array([-0.423879  ,  0.01845485, -0.5596409 , -0.84833058, -0.24737607,\n",
       "         0.18218593,  0.25824744,  0.06347825, -0.52746832, -0.25563315,\n",
       "        -0.04755123,  0.59254624, -0.33801098, -0.9102525 , -0.40943089,\n",
       "         0.07774633, -0.1818896 ,  0.26879047, -0.83853889,  0.09878642,\n",
       "         0.29272675, -0.7182208 , -0.04322398,  0.04401816,  0.1806052 ,\n",
       "        -0.2552809 , -0.07925575,  0.33012545, -0.58728217, -0.22512842,\n",
       "         0.22188745, -0.85839211, -0.06736808, -0.30164881,  0.04714314,\n",
       "        -0.20387728, -0.40365288,  0.06362453,  0.0252496 , -0.29921464,\n",
       "         0.44889216,  0.30607982, -0.14136962, -0.94044822, -0.13717291,\n",
       "         0.36186707,  0.84602538, -0.65770587,  0.90725832, -0.17866948])]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_embedding))\n",
    "print(len(corpus_embedding[0]))\n",
    "corpus_embedding[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:30<00:00, 659.51it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_embedding_test = []\n",
    "for sentence in tqdm(X_test.values):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    count_words = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            sentence_embedding += embedding\n",
    "            count_words += 1\n",
    "    if count_words != 0:\n",
    "        sentence_embedding /= count_words\n",
    "    corpus_embedding_test.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.34137867,  0.32644061, -0.6913592 ,  0.93042797, -0.24674746,\n",
       "        -0.00793737,  0.83434894,  0.42846851, -0.46822411,  0.18145308,\n",
       "         0.02695534,  0.28545126,  0.44953037,  0.20109814,  0.11302191,\n",
       "         0.18967716, -1.05604081, -0.12670688,  0.56329733, -0.32371481,\n",
       "         0.22560349,  0.31994825,  0.37654363, -0.04400121, -0.54762108,\n",
       "         0.5262587 , -0.66112728, -0.25609161, -0.53637749,  0.06435166,\n",
       "         0.03610195, -0.26324159, -0.07435891,  0.39390961, -0.28741728,\n",
       "        -0.74851899,  0.38042809,  0.35705272, -0.76367375,  0.00314158,\n",
       "         0.23122552, -0.44792692, -0.02893383, -0.63975109,  0.28802527,\n",
       "        -0.17319469,  0.57268161,  0.14568441,  1.24386435, -0.01803521]),\n",
       " array([ 0.35155937,  0.13444979, -0.30987932,  0.76400572,  0.09771357,\n",
       "        -0.0548918 ,  0.27184974,  0.26068094,  0.19945618,  0.07570894,\n",
       "         0.44851215, -0.0939522 ,  0.27898915,  0.03982248,  0.00264986,\n",
       "        -0.03002163, -0.94737078,  0.3978112 ,  0.31174569,  0.35780795,\n",
       "         0.2733596 , -0.69187058,  0.01855948, -0.40936985, -0.2645473 ,\n",
       "         0.45625717, -0.54760136,  0.09878182, -0.27050531,  0.44697059,\n",
       "         0.00171889, -0.9733659 , -0.25331898,  0.37458357, -0.38994234,\n",
       "        -0.2939401 ,  0.32683704,  0.28713381, -0.69962512, -0.33762442,\n",
       "         0.61482154, -0.03645043, -0.30078352, -0.4359485 , -0.09561943,\n",
       "         0.04120284,  0.56671915,  0.06486011,  0.80051835,  0.02740395])]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_embedding_test))\n",
    "print(len(corpus_embedding_test[0]))\n",
    "corpus_embedding_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:32<00:00, 619.80it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_embedding_cv = []\n",
    "for sentence in tqdm(X_cv.values):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    count_words = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            sentence_embedding += embedding\n",
    "            count_words += 1\n",
    "    if count_words != 0:\n",
    "        sentence_embedding /= count_words\n",
    "    corpus_embedding_cv.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.48616667,  0.18453874,  0.13360501,  0.49358281, -0.33790518,\n",
       "         0.3935452 ,  0.17377513, -0.10749542, -0.43118972,  0.35650193,\n",
       "        -0.17375508,  0.70026264, -1.51880368, -0.24281143,  0.64329223,\n",
       "         0.41680116,  0.2482118 ,  0.10048615,  0.77865304, -0.35422269,\n",
       "        -0.19091147,  0.72229682, -0.27655892, -0.6805847 , -1.58424106,\n",
       "        -0.24681489,  0.01320604,  0.44208384,  0.38081266, -0.21585519,\n",
       "        -0.71953296,  0.33142461, -1.55691555,  0.32053394,  0.39746836,\n",
       "         0.6322432 ,  0.83148427,  1.34785175,  0.00430632, -0.13085715,\n",
       "         0.45700192, -0.85015313,  0.16664648, -0.35655605,  0.39456441,\n",
       "         0.35720955,  1.03568124, -1.00298616,  1.36411868,  0.12226187]),\n",
       " array([ 0.05629636,  0.13720081, -0.5643377 ,  0.73348429, -0.22710556,\n",
       "         0.68583415,  0.7762212 ,  0.32816233, -0.58972414,  0.09781129,\n",
       "         0.08961729,  0.71635393, -0.40924045,  0.55642931, -0.27097759,\n",
       "         0.72200025,  0.2880372 ,  0.57930572,  0.28162473, -0.85692971,\n",
       "        -0.20225949,  0.19564505, -0.25243004, -0.20252654, -0.88684943,\n",
       "         1.13882169, -0.03933239,  0.25977055,  0.71070425,  0.15034778,\n",
       "         0.34067624,  0.56146291, -1.27763456,  0.45318277,  0.01289343,\n",
       "        -0.47714085,  0.16191057,  0.39411491,  0.02567499, -0.2518257 ,\n",
       "        -0.30258443, -0.38698142, -0.48992478, -0.3601316 , -0.0376675 ,\n",
       "         0.2742941 ,  0.39346672,  0.32672875,  0.44104632, -0.70549213])]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(corpus_embedding_cv))\n",
    "print(len(corpus_embedding_cv[0]))\n",
    "corpus_embedding_cv[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_DT = corpus_embedding\n",
    "X_cv_DT = corpus_embedding_cv\n",
    "X_test_DT = corpus_embedding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = list(np.arange(1,11))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "for depth in myList:\n",
    "    DT = DecisionTreeClassifier(max_depth = depth)\n",
    "    scores = cross_val_score(DT, X_train, y_train, cv = 5, scoring = 'f1_weighted')\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "# changing to misclassification error\n",
    "scr = [x for x in cv_scores]\n",
    "\n",
    "optimal_depth = myList[scr.index(max(scr))]\n",
    "print('\\nThe optimal depth is %d.' % optimal_depth)\n",
    "\n",
    "plt.plot(myList, scr)\n",
    "\n",
    "plt.xlabel('Depth value')\n",
    "plt.ylabel('f1-score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'bought sever vital can dog food product found good qualiti product look like stew process meat smell better labrador finicki appreci product better',\n",
       "       b'product arriv label jumbo salt peanutsth peanut actual small size unsalt sure error vendor intend repres product jumbo',\n",
       "       b'confect around centuri light pillowi citrus gelatin nut case filbert cut tini squar liber coat powder sugar tini mouth heaven chewi flavor high recommend yummi treat familiar stori lewi lion witch wardrob treat seduc edmund sell brother sister witch',\n",
       "       b'look secret ingredi robitussin believ found got addit root beer extract order good made cherri soda flavor medicin',\n",
       "       b'great taffi great price wide assort yummi taffi deliveri quick taffi lover deal',\n",
       "       b'got wild hair taffi order five pound bag taffi enjoy mani flavor watermelon root beer melon peppermint grape etc complaint bit much redblack licoriceflavor piec particular favorit kid husband last two week would recommend brand taffi delight treat',\n",
       "       b'saltwat taffi great flavor soft chewi candi individu wrap well none candi stuck togeth happen expens version fraling would high recommend candi serv beachthem parti everyon love',\n",
       "       b'taffi good soft chewi flavor amaz would definit recommend buy satisfi',\n",
       "       b'right most sprout cat eat grass love rotat around wheatgrass rye',\n",
       "       b'healthi dog food good digest also good small puppi dog eat requir amount everi feed',\n",
       "       b'dont know cactus tequila uniqu combin ingredi flavour hot sauc make one kind pick bottl trip brought back home total blown away realiz simpli couldnt find anywher citi bum magic internet case sauc ecstat love hot saucei mean realli love hot sauc dont want sauc tasteless burn throat grab bottl tequila picant gourmet inclan realiz tast never want use sauc thank person incred servic',\n",
       "       b'one boy need lose weight didnt put food floor chubbi guy proteinrich byproduct food higher skinni boy jump higher food sit go stale realli food chubbi boy lose ounc week',\n",
       "       b'cat happili eat felida platinum two year got new bag shape food differ tri new food first put bowl bowl sit full kitti touch food ive notic similar review relat formula chang past unfortun need find new food cat eat',\n",
       "       b'good flavor came secur pack fresh delici love twizzler',\n",
       "       b'strawberri twizzler guilti pleasur yummi six pound around son',\n",
       "       b'daughter love twizzler shipment six pound realli hit spot exact would expectsix packag strawberri twizzler',\n",
       "       b'love eat good watch look movi sweet like transfer zip lock baggi stay fresh take time eat',\n",
       "       b'satisfi twizzler purchas share other enjoy definit order',\n",
       "       b'twizzler strawberri childhood favorit candi made lancast pennsylvania candi inc one oldest confectioneri firm unit state subsidiari hershey compani compani establish young smyli also make appl licoric twist green color blue raspberri licoric twist like keep dri cool place recommend put fridg accord guin book record longest licoric twist ever made measur feet weight pound made candi inc recordbreak twist becam guin world record juli product kosher thank',\n",
       "       b'candi deliv fast purchas reason price home bound unabl get store perfect',\n",
       "       b'husband twizzler addict weve bought mani time amazon govern employe live oversea cant get countri assign theyv alway fresh tasti pack well arriv time manner',\n",
       "       b'bought husband current oversea love appar staff like also generous amount twizzler bag well worth price twizzler strawberri bag pack',\n",
       "       b'rememb buy candi kid qualiti hasnt drop year still superb product wont disappoint',\n",
       "       b'love candi weight watcher cut back still crave',\n",
       "       b'live yrs miss twizzler back visit someon visit alway stock say yum sell mexico faith buyer often abl buy right',\n",
       "       b'product receiv advertis twizzler strawberri bag pack',\n",
       "       b'candi red flavor plan chewi would never buy',\n",
       "       b'glad amazon carri batteri hard time find elsewher uniqu size need garag door open great deal price',\n",
       "       b'got mum diabet need watch sugar intak father simpli choos limit unnecessari sugar intak shes one sweet tooth love toffe would never guess theyr sugarfre great eat pretti much guilt free impress ive order dark chocol take offic ill eat instead snack sugari sweet excel',\n",
       "       b'never huge coffe fan howev mother purchas littl machin talk tri latt macciato coffe shop better one like product usual noncoffe drinker littl dolch guesto machin super easi use prepar realli good coffeelattecappuccinoetc less minut water heat would recommend dolc gusto anyon good price iam get one',\n",
       "       b'offer great price great tast thank amazon sell product staral',\n",
       "       b'mccann instant oatmeal great must oatmeal scrape togeth two three minut prepar escap fact howev even best instant oatmeal nowher near good even store brand oatmeal requir stovetop prepar still mccann good get instant oatmeal even better organ allnatur brand tri varieti mccann varieti pack tast good prepar microwav ad boil water conveni extrem time issu mccann use actual cane sugar instead high fructos corn syrup help decid buy product real sugar tast better harm stuff one thing like though mccann use thicken oat plus water plus heat make creami tasti oatmeal without need guar gum conveni product mayb guar gum sit bowl instant mccann becom thick gluey',\n",
       "       b'good instant oatmeal best oatmeal brand use cane sugar instead high fructous corn syrup better sweet doctor say form sugar better great cold morn dont time make mccann steel cut oat appl cinnamon best mapl brown sugar regular good plus dont requir doctor actual tell three flavor apart',\n",
       "       b'instant oatmeal becom soggi minut water hit bowl mccann instant oatmeal hold textur excel flavor good time mccann regular oat meal excel may take bit longer prepar time morn best instant brand ive ever eaten close second noninst varieti mccann instant irish oatmeal varieti pack regular appl cinnamon mapl brown sugar box pack',\n",
       "       b'mccann instant irish oatmeal varieti pack regular appl cinnamon mapl brown sugar box pack fan mccann steelcut oat thought give instant varieti tri found hardi meal sweet great folk like postbariatr surgeri need food palat easili digest fiber wont make bloat',\n",
       "       b'celiac diseas product lifesav could better get almost half price groceri health food store love mccann instant oatmeal flavor thank abbi',\n",
       "       b'els need know oatmeal instant make half cup lowfat milk add raisinsnuk second expens kroger store brand oatmeal mayb littl tastier better textur someth still oatmeal mmm conveni',\n",
       "       b'visit friend nate morn coffe came storag room packet mccann instant irish oatmeal suggest tri use stash sometim nate dose give chanc say end tri appl cinn found tasteful made water powder milk goe good coffe slice toast readi take worldor day least jerri reith',\n",
       "       b'order wife reccomend daughter almost everi morn like flavor shes happi happi mccann instant irish oatmeal varieti pack regular appl cinnamon mapl brown sugar box pack',\n",
       "       b'varieti pack tast great everi morn cent per meal dont understand everyon earth isnt buy stuff mapl brown sugar terrif follow appl cinnamon follow regular dont get tire ole thing tast great boil water small pot empti packet bowl pour boil water watch expand size tast realli good take minut prepar sure everyon earth isnt conveni healthi quick excel qualiti extrem cheap',\n",
       "       b'mccann make oatmeal everi oatmeal connoisseur whether one like raw pellet state cook half hour sloth addl instant done microwav three minut good that sure beauti instant varieti avail differ flavor well regular varieti pack allow differ tast explor well give chanc experi differ mccann wellknown oatmeal person like mccann cook thicker bodi top brand america appl cinnamon though tend littl liquidi may want experi amount water add microwav oatmeal cook one minut twentyseven second also watch get handl much time water use bad thing consid bad thing offer buy lot youll end six tencount box good whole famili oatmealeat your singl person alon well love oatmeal',\n",
       "       b'mccann oatmeal everi morn order amazon abl save almost per box great product tast great healthi',\n",
       "       b'mccann oatmeal good qualiti choic favorit appl cinnamon find none over sugari good hot breakfast minut excel',\n",
       "       b'realli like mccann steel cut oat find dont cook often tast much better groceri store brand conveni anyth keep eat oatmeal regular good thing',\n",
       "       b'good oatmeal like appl cinnamon best though wouldnt follow direct packag sinc alway come soupi tast could sinc like oatmeal realli thick add milk top',\n",
       "       b'realli like mapl brown sugar flavor regular fine brown sugar ad appl cinnamon flavor quick easi satisfi breakfast ill order brand varieti ill get mapl brown sugar',\n",
       "       b'oatmeal good mushi soft dont like quaker oat way',\n",
       "       b'got free packag bottl bloodi mari mix bought seller advertis workedlol tri share buddi love buy notic review yet well hot burn mouth forev hot nice temp perfect',\n",
       "       b'wasnt stock last time look vermont countri store weston find along jaw harp cranberri horseradish sauc fartless black bean salsa appl cider jelli newton cradl art motion stapl vermont mapl syrup back ass kickin peanut hot activ perspir gland behind ear arm requir beverag advertis glass cold milk box kleenex sinc make nose run look like ordinari peanut alreadi give idea work suspect peopl hit goodi absenc especi colleagu greg go take work earliest opportun empti content ordinari planter peanut see whose cri whose nose run return shaken ensur spice even distribut import wash hand consumpt touch eye youll nut asskickin peanut share peanut deliber ill probabl give greg jaw harp christma hell insult',\n",
       "       b'roast home stovetop popcorn popper outsid cours bean coffe bean direct green mexican altura seem wellsuit method first second crack distinct ive roast bean medium slight dark great result everi time aroma strong persist tast smooth velveti yet live'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Word2Vec with tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = TfidfVectorizer()\n",
    "\n",
    "tf_idf_matrix = model.fit_transform(X_train.values)\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:49<00:00, 547.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_embedding = []\n",
    "for sentence in tqdm(X_train):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * sentence.count(word)\n",
    "            sentence_embedding += (embedding * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_embedding /= weight_sum\n",
    "    tfidf_embedding.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.49783587,  0.12231475, -0.12560517,  0.36890145,  0.19909141,\n",
       "        -0.40510795, -0.28291334, -0.18040224,  0.23266884, -0.47090244,\n",
       "         0.4803518 ,  0.22619316, -0.29317434,  0.02209257,  0.08068267,\n",
       "        -0.29437718,  0.05540543,  0.17686955, -0.13521102, -0.12169355,\n",
       "        -0.36893002, -0.56393425,  0.63908845, -0.07714613, -0.30687754,\n",
       "         0.08690704, -0.29011346,  0.18253315, -0.88322875, -0.49945644,\n",
       "        -0.32704492, -1.01306613,  0.06844313,  0.36794265, -0.20144012,\n",
       "        -0.54565518, -0.28555346,  0.37439441,  0.83892591, -0.52325959,\n",
       "         0.60660107, -0.03196828,  0.01574486, -0.5170446 , -0.53218022,\n",
       "         0.37927397,  1.00704537, -0.5859586 ,  0.8077709 ,  0.03277443])]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_embedding[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:35<00:00, 560.64it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_embedding_test = []\n",
    "for sentence in tqdm(X_test):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * sentence.count(word)\n",
    "            sentence_embedding += (embedding * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_embedding /= weight_sum\n",
    "    tfidf_embedding_test.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.25171986,  0.23622187, -0.76169539,  0.68036085, -0.56270203,\n",
       "        -0.207025  ,  0.99290651,  0.51128945, -0.49077345,  0.25647373,\n",
       "        -0.42178422,  0.50763507,  0.57167984,  0.27064421, -0.01302953,\n",
       "         0.15699311, -0.83758979, -0.00754108,  0.62960362, -0.47149934,\n",
       "         0.07118557,  0.37806763,  0.48500558,  0.11890137, -0.56241435,\n",
       "         0.38334651, -0.66468657, -0.2712106 , -0.32648738, -0.1279903 ,\n",
       "        -0.07356892, -0.10431867, -0.01893302,  0.64409424, -0.27912134,\n",
       "        -0.64605565,  0.39366062,  0.22463009, -0.43928205,  0.13364965,\n",
       "         0.09099705, -0.50904631, -0.13208658, -0.49465468,  0.24093296,\n",
       "        -0.13767485,  0.68397224,  0.13954011,  1.21340579, -0.01779093])]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_embedding_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:38<00:00, 526.00it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_embedding_cv = []\n",
    "for sentence in tqdm(X_cv):\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    sentence_embedding = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence.split():\n",
    "        if word in w2v_vocab:\n",
    "            embedding = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * sentence.count(word)\n",
    "            sentence_embedding += (embedding * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_embedding /= weight_sum\n",
    "    tfidf_embedding_cv.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.70055491,  0.23255103,  0.20840896,  0.22608537, -0.46473457,\n",
       "         0.46686146, -0.09497256, -0.2967896 , -0.37357113,  0.55676659,\n",
       "        -0.48692695,  0.94216948, -1.79095767, -0.53577616,  0.66806717,\n",
       "         0.58600075,  0.4351291 ,  0.33036784,  0.95311044, -0.26563092,\n",
       "        -0.35177702,  0.8849274 , -0.42681981, -0.70409937, -1.95830514,\n",
       "        -0.46036565,  0.0285272 ,  0.62639581,  0.52375936, -0.27434186,\n",
       "        -0.93943526,  0.33869912, -1.86096169,  0.3231114 ,  0.55746352,\n",
       "         0.75543446,  0.95196995,  1.46532887,  0.19842415, -0.09156518,\n",
       "         0.68453445, -1.08542465,  0.13417033, -0.12654752,  0.62075355,\n",
       "         0.32465316,  1.17428756, -0.91031479,  1.5900295 ,  0.42382099])]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_embedding_cv[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Text\"]\n",
    "y = df[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:2000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,) (1600,) (400,) (1200,) (400,)\n"
     ]
    }
   ],
   "source": [
    "X_1 = X[:math.ceil(len(df)*.8)]\n",
    "y_1 = y[:math.ceil(len(df)*.8)]\n",
    "X_test = X[math.ceil(len(df)*.8):]\n",
    "y_test = y[math.ceil(len(df)*.8):]\n",
    "\n",
    "X_train = X_1[:math.ceil(len(df)*.6)]\n",
    "y_train = y_1[:math.ceil(len(df)*.6)]\n",
    "X_cv = X_1[math.ceil(len(df)*.6):]\n",
    "y_cv = y_1[math.ceil(len(df)*.6):]\n",
    "\n",
    "print(X_1.shape, y_1.shape, y_test.shape, X_train.shape, X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "X_train = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vect.transform(X_test)\n",
    "X_cv = count_vect.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1200x4899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X[:math.ceil(len(df)*.8)]\n",
    "y_1 = y[:math.ceil(len(df)*.8)]\n",
    "X_test = X[math.ceil(len(df)*.8):]\n",
    "y_test = y[math.ceil(len(df)*.8):]\n",
    "\n",
    "X_train = X_1[:math.ceil(len(df)*.6)]\n",
    "y_train = y_1[:math.ceil(len(df)*.6)]\n",
    "X_cv = X_1[math.ceil(len(df)*.6):]\n",
    "y_cv = y_1[math.ceil(len(df)*.6):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "X_train = tf_idf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf_idf_vect.transform(X_test)\n",
    "X_cv = tf_idf_vect.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1200x4899 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
